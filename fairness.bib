
@article{friedler_im_2016,
	title = {On the (im) possibility of fairness},
	url = {https://arxiv.org/abs/1609.07236},
	urldate = {2016-11-09},
	journal = {arXiv preprint arXiv:1609.07236},
	author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	year = {2016},
	keywords = {read, theory},
	file = {1609.07236v1.pdf:/home/hargup/zotero/storage/HFM7TZUW/1609.07236v1.pdf:application/pdf}
}

@book{mayer-schonberger_big_2014,
	address = {Boston},
	edition = {First Mariner Books edition},
	title = {Big data: a revolution that will transform how we live, work, and think},
	isbn = {978-0-544-22775-0},
	shorttitle = {Big data},
	publisher = {Mariner Books, Houghton Mifflin Harcourt},
	author = {Mayer-Schönberger, Viktor and Cukier, Kenneth},
	year = {2014},
	note = {OCLC: ocn854944359},
	keywords = {Big data, Data mining, Electronic information resources, Internet, Social aspects, Social change, Technological innovations},
	file = {1412.4643v3.pdf:/home/hargup/zotero/storage/FNNFMMQF/1412.4643v3.pdf:application/pdf}
}

@article{khandani_consumer_2010,
	title = {Consumer credit-risk models via machine-learning algorithms},
	volume = {34},
	issn = {03784266},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0378426610002372},
	doi = {10.1016/j.jbankfin.2010.06.001},
	language = {en},
	number = {11},
	urldate = {2016-11-09},
	journal = {Journal of Banking \& Finance},
	author = {Khandani, Amir E. and Kim, Adlar J. and Lo, Andrew W.},
	month = nov,
	year = {2010},
	pages = {2767--2787},
	file = {Lo JBF Pub abstract and paper.pdf:/home/hargup/zotero/storage/CKXBIX9E/Lo JBF Pub abstract and paper.pdf:application/pdf}
}

@book{perry_predictive_2013,
	address = {Santa Monica, CA},
	title = {Predictive policing: the role of crime forecasting in law enforcement operations},
	isbn = {978-0-8330-8148-3},
	shorttitle = {Predictive policing},
	publisher = {RAND},
	author = {Perry, Walt L.},
	year = {2013},
	keywords = {Crime forecasting, Crime prevention, Criminal behavior, Prediction of, Police, United States},
	file = {RAND_RR233.pdf:/home/hargup/zotero/storage/EX7ED2CC/RAND_RR233.pdf:application/pdf}
}

@article{freedman_note_1983,
	title = {A {Note} on {Screening} {Regression} {Equations}},
	volume = {37},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1983.10482729},
	doi = {10.1080/00031305.1983.10482729},
	language = {en},
	number = {2},
	urldate = {2016-11-09},
	journal = {The American Statistician},
	author = {Freedman, David A. and Freedman, David A.},
	month = may,
	year = {1983},
	keywords = {stats},
	pages = {152--155},
	file = {screening_regression_equations.pdf:/home/hargup/zotero/storage/8FRHMC4D/screening_regression_equations.pdf:application/pdf}
}

@article{dhir_crime_2016,
	title = {Crime and {Criminal} {Tracking} {Networks} \& {Systems} {Using} {Agile} {Methodology}},
	volume = {8},
	url = {http://bvicam.ac.in/bijit/downloads/pdf/issue15/02.pdf},
	number = {1},
	urldate = {2016-11-09},
	journal = {BVICAM's International Journal of Information Technology},
	author = {Dhir, Saru and Sarraf, Swati},
	year = {2016},
	file = {02.pdf:/home/hargup/zotero/storage/57ZXT2VN/02.pdf:application/pdf}
}

@article{sparrow_new_2009,
	title = {New {Perspectives} in {Policing}},
	url = {https://www.hks.harvard.edu/fs/msparrow/documents--in%20use/Measuring%20Performance%20in%20a%20Modern%20Police%20Organization--NIJ%20Perspectives%20in%20Policing--March%202015.pdf},
	urldate = {2016-11-09},
	author = {Sparrow, Malcolm K.},
	year = {2009},
	file = {248476.pdf:/home/hargup/zotero/storage/K5SSXD2D/248476.pdf:application/pdf}
}

@inproceedings{kumar_data_2015,
	title = {Data mining based crime investigation systems: {Taxonomy} and relevance},
	shorttitle = {Data mining based crime investigation systems},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7342782},
	urldate = {2016-11-09},
	booktitle = {Communication {Technologies} ({GCCT}), 2015 {Global} {Conference} on},
	publisher = {IEEE},
	author = {Kumar, Arunima S. and Gopal, Raju K.},
	year = {2015},
	pages = {850--853},
	file = {07342782.pdf:/home/hargup/zotero/storage/6QCNV3E4/07342782.pdf:application/pdf}
}

@article{rieder_datatrust:_2016,
	title = {Datatrust: {Or}, the political quest for numerical evidence and the epistemologies of {Big} {Data}},
	volume = {3},
	issn = {2053-9517},
	shorttitle = {Datatrust},
	url = {http://bds.sagepub.com/lookup/doi/10.1177/2053951716649398},
	doi = {10.1177/2053951716649398},
	language = {en},
	number = {1},
	urldate = {2016-11-09},
	journal = {Big Data \& Society},
	author = {Rieder, G. and Simon, J.},
	month = jun,
	year = {2016},
	file = {2053951716649398.full(1).pdf:/home/hargup/zotero/storage/EV85UWFI/2053951716649398.full(1).pdf:application/pdf}
}

@article{saunders_predictions_2016,
	title = {Predictions put into practice: a quasi-experimental evaluation of {Chicago}’s predictive policing pilot},
	volume = {12},
	issn = {1573-3750, 1572-8315},
	shorttitle = {Predictions put into practice},
	url = {http://link.springer.com/10.1007/s11292-016-9272-0},
	doi = {10.1007/s11292-016-9272-0},
	language = {en},
	number = {3},
	urldate = {2016-11-09},
	journal = {Journal of Experimental Criminology},
	author = {Saunders, Jessica and Hunt, Priscillia and Hollywood, John S.},
	month = sep,
	year = {2016},
	pages = {347--371},
	file = {art%3A10.1007%2Fs11292-016-9272-0.pdf:/home/hargup/zotero/storage/44TTJ2DA/art%3A10.1007%2Fs11292-016-9272-0.pdf:application/pdf}
}

@article{schein_bayesian_2016,
	title = {Bayesian {Poisson} {Tucker} {Decomposition} for {Learning} the {Structure} of {International} {Relations}},
	url = {http://arxiv.org/abs/1606.01855},
	urldate = {2016-11-09},
	journal = {arXiv preprint arXiv:1606.01855},
	author = {Schein, Aaron and Zhou, Mingyuan and Blei, David M. and Wallach, Hanna},
	year = {2016},
	file = {bayesian_international_relations.pdf:/home/hargup/zotero/storage/QETWDB52/bayesian_international_relations.pdf:application/pdf}
}

@article{berendt_better_2014,
	title = {Better decision support through exploratory discrimination-aware data mining: foundations and empirical evidence},
	volume = {22},
	issn = {0924-8463, 1572-8382},
	shorttitle = {Better decision support through exploratory discrimination-aware data mining},
	url = {http://link.springer.com/10.1007/s10506-013-9152-0},
	doi = {10.1007/s10506-013-9152-0},
	language = {en},
	number = {2},
	urldate = {2016-11-09},
	journal = {Artificial Intelligence and Law},
	author = {Berendt, Bettina and Preibusch, Sören},
	month = jun,
	year = {2014},
	keywords = {bias correction},
	pages = {175--209},
	file = {better_decision_support_through_discrimination_aware_data_mining.pdf:/home/hargup/zotero/storage/FP9U2UFX/better_decision_support_through_discrimination_aware_data_mining.pdf:application/pdf}
}

@article{ferguson_countering_2016,
	title = {Countering violent extremism through media and communication strategies},
	volume = {27},
	url = {http://www.paccsresearch.org.uk/wp-content/uploads/2016/03/Countering-Violent-Extremism-Through-Media-and-Communication-Strategies-.pdf},
	urldate = {2016-11-09},
	journal = {Reflections},
	author = {Ferguson, Kate},
	year = {2016},
	pages = {28},
	file = {Countering-Violent-Extremism-Through-Media-and-Communication-Strategies-.pdf:/home/hargup/zotero/storage/2CDS9T3W/Countering-Violent-Extremism-Through-Media-and-Communication-Strategies-.pdf:application/pdf}
}

@article{boyd_critical_2012,
	title = {{CRITICAL} {QUESTIONS} {FOR} {BIG} {DATA}: {Provocations} for a cultural, technological, and scholarly phenomenon},
	volume = {15},
	issn = {1369-118X, 1468-4462},
	shorttitle = {{CRITICAL} {QUESTIONS} {FOR} {BIG} {DATA}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/1369118X.2012.678878},
	doi = {10.1080/1369118X.2012.678878},
	language = {en},
	number = {5},
	urldate = {2016-11-09},
	journal = {Information, Communication \& Society},
	author = {boyd, danah and Crawford, Kate},
	month = jun,
	year = {2012},
	pages = {662--679},
	file = {CRITICAL QUESTIONS FOR BIG DATA.pdf:/home/hargup/zotero/storage/RJSADRCB/CRITICAL QUESTIONS FOR BIG DATA.pdf:application/pdf}
}

@article{tramer_fairtest:_2015,
	title = {{FairTest}: {Discovering} {Unwarranted} {Associations} in {Data}-{Driven} {Applications}},
	shorttitle = {{FairTest}},
	url = {https://arxiv.org/abs/1510.02377},
	urldate = {2016-11-09},
	journal = {arXiv preprint arXiv:1510.02377},
	author = {Tramèr, Florian and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Hubaux, Jean-Pierre and Humbert, Mathias and Juels, Ari and Lin, Huang},
	year = {2015},
	file = {fair-test.pdf:/home/hargup/zotero/storage/5TA7JZPP/fair-test.pdf:application/pdf}
}

@article{shorey_toward_2011,
	title = {Toward a {Framework} for the {Large} {Scale} {Textual} and {Contextual} {Analysis} of {Government} {Information} {Declassification} {Patterns}},
	url = {https://works.bepress.com/hanna_wallach/15/},
	urldate = {2016-11-09},
	author = {Shorey, Rachel and Wallach, Hanna M. and Demarais, Bruce},
	year = {2011},
	file = {Framework_for_textual_analysis_of_government_declassifications.pdf:/home/hargup/zotero/storage/VZCXN75P/Framework_for_textual_analysis_of_government_declassifications.pdf:application/pdf}
}

@article{metcalf_where_2016,
	title = {Where are {Human} {Subjects} in {Big} {Data} {Research}? {The} {Emerging} {Ethics} {Divide}},
	shorttitle = {Where are {Human} {Subjects} in {Big} {Data} {Research}?},
	url = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2779647},
	urldate = {2016-11-09},
	journal = {The Emerging Ethics Divide (May 14, 2016). Big Data and Society, Spring},
	author = {Metcalf, Jacob and Crawford, Kate},
	year = {2016},
	file = {human_subjects_in_big_data_research.pdf:/home/hargup/zotero/storage/UPCH8R2H/human_subjects_in_big_data_research.pdf:application/pdf}
}

@article{gupta_framework_2014,
	title = {A framework of intelligent decision support system for {Indian} police},
	volume = {27},
	issn = {1741-0398},
	url = {http://www.emeraldinsight.com/doi/10.1108/JEIM-10-2012-0073},
	doi = {10.1108/JEIM-10-2012-0073},
	language = {en},
	number = {5},
	urldate = {2016-11-09},
	journal = {Journal of Enterprise Information Management},
	author = {Gupta, Manish and Chandra, B. and Gupta, M.P.},
	month = sep,
	year = {2014},
	keywords = {predictive policing},
	pages = {512--540},
	file = {JEIM-10-2012-0073.pdf:/home/hargup/zotero/storage/NJA2UXCH/JEIM-10-2012-0073.pdf:application/pdf}
}

@article{zliobaite_survey_2015,
	title = {A survey on measuring indirect discrimination in machine learning},
	url = {http://arxiv.org/abs/1511.00148},
	urldate = {2016-11-09},
	journal = {arXiv preprint arXiv:1511.00148},
	author = {Zliobaite, Indre},
	year = {2015},
	keywords = {bias correction},
	file = {ml_discrimination_survey.pdf:/home/hargup/zotero/storage/IE3G3D4V/ml_discrimination_survey.pdf:application/pdf}
}

@article{goodman_european_2016,
	title = {European {Union} regulations on algorithmic decision-making and a" right to explanation"},
	url = {https://arxiv.org/abs/1606.08813},
	urldate = {2016-11-09},
	journal = {arXiv preprint arXiv:1606.08813},
	author = {Goodman, Bryce and Flaxman, Seth},
	year = {2016},
	keywords = {law, read},
	file = {right_to_explaination.pdf:/home/hargup/zotero/storage/W4W5U5VW/right_to_explaination.pdf:application/pdf}
}

@article{scurich_quantifying_2016,
	title = {Quantifying the presumption of innocence},
	volume = {15},
	issn = {1470-8396, 1470-840X},
	url = {http://lpr.oxfordjournals.org/lookup/doi/10.1093/lpr/mgv016},
	doi = {10.1093/lpr/mgv016},
	language = {en},
	number = {1},
	urldate = {2016-11-09},
	journal = {Law, Probability and Risk},
	author = {Scurich, Nicholas and Nguyen, Kenneth D. and John, Richard S.},
	month = mar,
	year = {2016},
	pages = {71--86},
	file = {scurich2015.pdf:/home/hargup/zotero/storage/VCC9CGZ7/scurich2015.pdf:application/pdf}
}

@article{crawford_networks_2013,
	title = {Networks of governance: users, platforms, and the challenges of networked media regulation},
	volume = {1},
	shorttitle = {Networks of governance},
	url = {http://www.inderscienceonline.com/doi/abs/10.1504/IJTPL.2013.057008},
	number = {3},
	urldate = {2016-11-09},
	journal = {International Journal of Technology Policy and Law},
	author = {Crawford, Kate and Lumby, Catharine},
	year = {2013},
	keywords = {law},
	pages = {270--282},
	file = {SSRN-id2246772.pdf:/home/hargup/zotero/storage/WFFKZD7K/SSRN-id2246772.pdf:application/pdf}
}

@article{ajunwa_limitless_2017,
	title = {Limitless {Worker} {Surveillance}},
	volume = {105},
	url = {http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2746211},
	number = {3},
	urldate = {2016-11-09},
	journal = {California Law Review},
	author = {Ajunwa, Ifeoma and Crawford, Kate and Schultz, Jason},
	year = {2017},
	file = {SSRN-id2746211.pdf:/home/hargup/zotero/storage/Z82A5U9D/SSRN-id2746211.pdf:application/pdf}
}

@article{bonchi_algorithmic_????,
	title = {Algorithmic bias: from discrimination discovery to fairness-aware data mining},
	shorttitle = {Algorithmic bias},
	url = {http://francescobonchi.com/tutorial-algorithmic-bias.pdf},
	urldate = {2016-11-09},
	author = {Bonchi, Francesco and Castillo, Carlos and Hajian, Sara},
	file = {tutorial-algorithmic-bias.pdf:/home/hargup/zotero/storage/HH4I3MZM/tutorial-algorithmic-bias.pdf:application/pdf}
}

@article{pradesh_crime_????,
	title = {Crime \& {Criminal} {Tracking} {Network} and {Systems} ({CCTNS})},
	url = {http://ncrb.nic.in/BureauDivisions/CCTNS/All%20State%20RFP/Uttar%20Pradesh/UP_CCTNS_SI_RFP_Volume_III.pdf},
	urldate = {2016-11-09},
	author = {Pradesh, Uttar},
	file = {UP_CCTNS_SI_RFP_Volume_III.pdf:/home/hargup/zotero/storage/48C6QB4I/UP_CCTNS_SI_RFP_Volume_III.pdf:application/pdf}
}

@misc{national_consumer_law_center_past_????,
	title = {Past {Imperfect}: {How} {Credit} {Scores} and {Other} {Analytics}  “{Bake} {In}” and {Perpetuate} {Past} {Discrimination}},
	author = {{National Consumer Law Center}},
	file = {Past_Imperfect050616.pdf:/home/hargup/zotero/storage/QMNQG8PE/Past_Imperfect050616.pdf:application/pdf}
}

@misc{_randomized_????,
	title = {Randomized controlled  field trials of predictive policing},
	note = {bibtex: \_randomized\_????},
	file = {MohlerEtAl-2015-JASA-Predictive-InPress.pdf:/home/hargup/zotero/storage/USXHH5VH/MohlerEtAl-2015-JASA-Predictive-InPress.pdf:application/pdf}
}

@misc{executive_office_of_the_president_big_????,
	title = {Big {Data}: {A} {Report} on  {Algorithmic} {Systems}, {Opportunity}, and {Civil} {Right}},
	author = {{Executive Office of the President}},
	file = {2016_0504_data_discrimination.pdf:/home/hargup/zotero/storage/XNN7NFJB/2016_0504_data_discrimination.pdf:application/pdf}
}

@misc{ftc_big_????,
	title = {Big {Data}: {A} {Tool} for {Inclusion} or {Exclusion}?},
	author = {{FTC}},
	file = {160106big-data-rpt.pdf:/home/hargup/zotero/storage/HT6DJRPK/160106big-data-rpt.pdf:application/pdf}
}

@misc{center_for_financial_services_institutions_big_????,
	title = {Big {Data}, {Big} {Potential}:  {Harnessing} {Data} {Technology}   for the {Underserved} {Market}},
	author = {{Center for Financial Services Institutions}},
	file = {Big_Data_Big_Potential.pdf:/home/hargup/zotero/storage/3FTM9DFK/Big_Data_Big_Potential.pdf:application/pdf}
}

@misc{thomas_jarchow_big_????,
	title = {Big {Data}: {Opportunities}, risks and  need for action by the {Confederation}},
	author = {{Thomas Jarchow} and {Beat Estermann}},
	file = {big_data_study_-summary.pdf:/home/hargup/zotero/storage/GQQDCR49/big_data_study_-summary.pdf:application/pdf}
}

@misc{national_consumer_law__center_big_????,
	title = {{BIG} {DATA} {A} {BIG} {DISAPPOINTMENT} {FOR} {SCORING}   {CONSUMER} {CREDIT} {RISK}},
	author = {{NATIONAL CONSUMER LAW  CENTER}},
	file = {report-big-data.pdf:/home/hargup/zotero/storage/49PKZT5K/report-big-data.pdf:application/pdf}
}

@misc{european_microfinance_networkinance_credit_????,
	title = {Credit {Scoring} in the {European}   ({Micro})finance {Sector}},
	author = {{European Microfinance Networkinance}},
	file = {SCORING_EMN2015_LINKS_2.pdf:/home/hargup/zotero/storage/3KFS6XBQ/SCORING_EMN2015_LINKS_2.pdf:application/pdf}
}

@misc{solon_barocas_big_????,
	title = {Big {Data}’s {Disparate} {Impact}},
	url = {ttp://dx.doi.org/10.15779/Z38BG3},
	author = {{Solon Barocas} and {Andrew D. Selbst}},
	note = {bibtex: solon\_barocas\_big\_????},
	keywords = {law, to read},
	file = {SSRN-id2477899.pdf:/home/hargup/zotero/storage/KWKWZMJ7/SSRN-id2477899.pdf:application/pdf}
}

@misc{kofman_how_2016,
	title = {How a {Facial} {Recognition} {Mismatch} {Can} {Ruin} {Your} {Life}},
	url = {https://theintercept.com/2016/10/13/how-a-facial-recognition-mismatch-can-ruin-your-life/},
	abstract = {FBI facial recognition experts identified Steve Talley as a bank robber. They were wrong.},
	urldate = {2016-11-09},
	journal = {The Intercept},
	author = {Kofman, Ava},
	year = {2016},
	note = {bibtex: kofman\_how\_2016},
	keywords = {bias example, read},
	file = {Snapshot:/home/hargup/zotero/storage/E73GSDVH/how-a-facial-recognition-mismatch-can-ruin-your-life.html:text/html}
}

@article{bolukbasi_man_2016,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
	url = {http://arxiv.org/abs/1607.06520},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	urldate = {2016-11-09},
	journal = {arXiv:1607.06520 [cs, stat]},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.06520},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning, read, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/hargup/zotero/storage/IV39QIFS/1607.html:text/html;Bolukbasi et al_2016_Man is to Computer Programmer as Woman is to Homemaker.pdf:/home/hargup/zotero/storage/55JI5K5M/Bolukbasi et al_2016_Man is to Computer Programmer as Woman is to Homemaker.pdf:application/pdf}
}

@inproceedings{kay_unequal_2015,
	address = {New York, NY, USA},
	series = {{CHI} '15},
	title = {Unequal {Representation} and {Gender} {Stereotypes} in {Image} {Search} {Results} for {Occupations}},
	isbn = {978-1-4503-3145-6},
	url = {http://doi.acm.org/10.1145/2702123.2702520},
	doi = {10.1145/2702123.2702520},
	abstract = {Information environments have the power to affect people's perceptions and behaviors. In this paper, we present the results of studies in which we characterize the gender bias present in image search results for a variety of occupations. We experimentally evaluate the effects of bias in image search results on the images people choose to represent those careers and on people's perceptions of the prevalence of men and women in each occupation. We find evidence for both stereotype exaggeration and systematic underrepresentation of women in search results. We also find that people rate search results higher when they are consistent with stereotypes for a career, and shifting the representation of gender in image search results can shift people's perceptions about real-world distributions. We also discuss tensions between desires for high-quality results and broader societal goals for equality of representation in this space.},
	urldate = {2016-11-09},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kay, Matthew and Matuszek, Cynthia and Munson, Sean A.},
	year = {2015},
	note = {bibtex: kay\_unequal\_2015},
	keywords = {bias, bias example, gender, image search, inequality, read, representation, stereotypes},
	pages = {3819--3828},
	file = {Kay et al_2015_Unequal Representation and Gender Stereotypes in Image Search Results for.pdf:/home/hargup/zotero/storage/58SFKEZC/Kay et al_2015_Unequal Representation and Gender Stereotypes in Image Search Results for.pdf:application/pdf;Kay et al_2015_Unequal Representation and Gender Stereotypes in Image Search Results for.pdf:/home/hargup/zotero/storage/UK2V5RMV/Kay et al_2015_Unequal Representation and Gender Stereotypes in Image Search Results for.pdf:application/pdf;Kay et al_2015_Unequal Representation and Gender Stereotypes in Image Search Results for.pdf:/home/hargup/zotero/storage/PFWNG5NH/Kay et al_2015_Unequal Representation and Gender Stereotypes in Image Search Results for.pdf:application/pdf}
}

@article{paul_why_2014,
	title = {Why does {Deep} {Learning} work? - {A} perspective from {Group} {Theory}},
	shorttitle = {Why does {Deep} {Learning} work?},
	url = {http://arxiv.org/abs/1412.6621},
	abstract = {Why does Deep Learning work? What representations does it capture? How do higher-order representations emerge? We study these questions from the perspective of group theory, thereby opening a new approach towards a theory of Deep learning. One factor behind the recent resurgence of the subject is a key algorithmic step called pre-training: first search for a good generative model for the input samples, and repeat the process one layer at a time. We show deeper implications of this simple principle, by establishing a connection with the interplay of orbits and stabilizers of group actions. Although the neural networks themselves may not form groups, we show the existence of \{{\textbackslash}em shadow\} groups whose elements serve as close approximations. Over the shadow groups, the pre-training step, originally introduced as a mechanism to better initialize a network, becomes equivalent to a search for features with minimal orbits. Intuitively, these features are in a way the \{{\textbackslash}em simplest\}. Which explains why a deep learning network learns simple features first. Next, we show how the same principle, when repeated in the deeper layers, can capture higher order representations, and why representation complexity increases as the layers get deeper.},
	urldate = {2016-11-11},
	journal = {arXiv:1412.6621 [cs, stat]},
	author = {Paul, Arnab and Venkatasubramanian, Suresh},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6621},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/hargup/zotero/storage/5V45EXKW/1412.html:text/html;Paul_Venkatasubramanian_2014_Why does Deep Learning work.pdf:/home/hargup/zotero/storage/4JHTAFZA/Paul_Venkatasubramanian_2014_Why does Deep Learning work.pdf:application/pdf}
}

@misc{angwin_machine_2016,
	title = {Machine {Bias}: {There}’s {Software} {Used} {Across} the {Country} to {Predict} {Future} {Criminals}. {And} it’s {Biased} {Against} {Blacks}.},
	shorttitle = {Machine {Bias}},
	url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
	abstract = {There’s software used across the country to predict future criminals. And it’s biased against blacks.},
	urldate = {2016-11-11},
	journal = {ProPublica},
	author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Kirchner},
	month = may,
	year = {2016},
	keywords = {bias example, read},
	file = {Snapshot:/home/hargup/zotero/storage/2SJ6CMFJ/machine-bias-risk-assessments-in-criminal-sentencing.html:text/html}
}

@article{sweeney_discrimination_2013,
	title = {Discrimination in {Online} {Ad} {Delivery}},
	volume = {11},
	issn = {1542-7730},
	url = {http://doi.acm.org/10.1145/2460276.2460278},
	doi = {10.1145/2460276.2460278},
	number = {3},
	urldate = {2016-11-11},
	journal = {Queue},
	author = {Sweeney, Latanya},
	month = mar,
	year = {2013},
	keywords = {bias example},
	pages = {10:10--10:29},
	file = {ACM Full Text PDF:/home/hargup/zotero/storage/I2D3UK87/Sweeney - 2013 - Discrimination in Online Ad Delivery.pdf:application/pdf}
}

@article{datta_automated_2015,
	title = {Automated {Experiments} on {Ad} {Privacy} {Settings}},
	volume = {2015},
	issn = {2299-0984},
	url = {https://www.degruyter.com/view/j/popets.2015.1.issue-1/popets-2015-0007/popets-2015-0007.xml?ncid=txtlnkusaolp00000618},
	doi = {10.1515/popets-2015-0007},
	abstract = {To partly address people’s concerns over web tracking, Google has created the Ad Settings webpage to provide information about and some choice over the profiles Google creates on users. We present AdFisher, an automated tool that explores how user behaviors, Google’s ads, and Ad Settings interact. AdFisher can run browser-based experiments and analyze data using machine learning and significance tests. Our tool uses a rigorous experimental design and statistical analysis to ensure the statistical soundness of our results. We use AdFisher to find that the Ad Settings was opaque about some features of a user’s profile, that it does provide some choice on ads, and that these choices can lead to seemingly discriminatory ads. In particular, we found that visiting webpages associated with substance abuse changed the ads shown but not the settings page. We also found that setting the gender to female resulted in getting fewer instances of an ad related to high paying jobs than setting it to male. We cannot determine who caused these findings due to our limited visibility into the ad ecosystem, which includes Google, advertisers, websites, and users. Nevertheless, these results can form the starting point for deeper investigations by either the companies themselves or by regulatory bodies.},
	number = {1},
	urldate = {2016-11-11},
	journal = {Proceedings on Privacy Enhancing Technologies},
	author = {Datta, Amit and Tschantz, Michael Carl and Datta, Anupam},
	year = {2015},
	keywords = {bias example},
	pages = {92--112},
	file = {[Proceedings on Privacy Enhancing Technologies] Automated Experiments on Ad Privacy Settings.pdf:/home/hargup/Downloads/[Proceedings on Privacy Enhancing Technologies] Automated Experiments on Ad Privacy Settings.pdf:application/pdf}
}

@inproceedings{beigman_learning_2009,
	address = {Stroudsburg, PA, USA},
	series = {{ACL} '09},
	title = {Learning with {Annotation} {Noise}},
	isbn = {978-1-932432-45-9},
	url = {http://dl.acm.org/citation.cfm?id=1687878.1687919},
	abstract = {It is usually assumed that the kind of noise existing in annotated data is random classification noise. Yet there is evidence that differences between annotators are not always random attention slips but could result from different biases towards the classification categories, at least for the harder-to-decide cases. Under an annotation generation model that takes this into account, there is a hazard that some of the training instances are actually hard cases with unreliable annotations. We show that these are relatively unproblematic for an algorithm operating under the 0--1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction on the uncontroversial cases at test time.},
	urldate = {2016-11-11},
	booktitle = {Proceedings of the {Joint} {Conference} of the 47th {Annual} {Meeting} of the {ACL} and the 4th {International} {Joint} {Conference} on {Natural} {Language} {Processing} of the {AFNLP}: {Volume} 1 - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Beigman, Eyal and Klebanov, Beata Beigman},
	year = {2009},
	pages = {280--287},
	file = {ACM Full Text PDF:/home/hargup/zotero/storage/NCAN2HVS/Beigman and Klebanov - 2009 - Learning with Annotation Noise.pdf:application/pdf}
}

@inproceedings{torralba_unbiased_2011,
	title = {Unbiased look at dataset bias},
	doi = {10.1109/CVPR.2011.5995347},
	abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
	booktitle = {2011 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Torralba, A. and Efros, A. A.},
	month = jun,
	year = {2011},
	keywords = {algorithm evaluation protocols, closed world assumption effects, Communities, contemporary object recognition, cross dataset generalization, data capture, Internet, object recognition, recognition datasets, relative data bias, sample value, Support vector machines, Testing, Training, visual databases, Visualization},
	pages = {1521--1528},
	file = {IEEE Xplore Abstract Record:/home/hargup/zotero/storage/I66HJP3A/5995347.html:text/html}
}

@inproceedings{pedreshi_discrimination-aware_2008,
	address = {New York, NY, USA},
	series = {{KDD} '08},
	title = {Discrimination-aware {Data} {Mining}},
	isbn = {978-1-60558-193-4},
	url = {http://doi.acm.org/10.1145/1401890.1401959},
	doi = {10.1145/1401890.1401959},
	urldate = {2016-11-11},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Pedreshi, Dino and Ruggieri, Salvatore and Turini, Franco},
	year = {2008},
	keywords = {classification rules, discrimination},
	pages = {560--568},
	file = {ACM Full Text PDF:/home/hargup/zotero/storage/IDVEP2TV/Pedreshi et al. - 2008 - Discrimination-aware Data Mining.pdf:application/pdf}
}

@inproceedings{feldman_certifying_2015,
	address = {New York, NY, USA},
	series = {{KDD} '15},
	title = {Certifying and {Removing} {Disparate} {Impact}},
	isbn = {978-1-4503-3664-2},
	url = {http://doi.acm.org/10.1145/2783258.2783311},
	doi = {10.1145/2783258.2783311},
	abstract = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process. When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses. We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
	urldate = {2016-11-11},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	year = {2015},
	keywords = {disparate impact, fairness, law, machine learning, to read},
	pages = {259--268},
	file = {ACM Full Text PDF:/home/hargup/zotero/storage/A7AQF2TN/Feldman et al. - 2015 - Certifying and Removing Disparate Impact.pdf:application/pdf}
}

@inproceedings{dwork_fairness_2012,
	address = {New York, NY, USA},
	series = {{ITCS} '12},
	title = {Fairness {Through} {Awareness}},
	isbn = {978-1-4503-1115-1},
	url = {http://doi.acm.org/10.1145/2090236.2090255},
	doi = {10.1145/2090236.2090255},
	abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	urldate = {2016-11-11},
	booktitle = {Proceedings of the 3rd {Innovations} in {Theoretical} {Computer} {Science} {Conference}},
	publisher = {ACM},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	year = {2012},
	pages = {214--226},
	file = {ACM Full Text PDF:/home/hargup/zotero/storage/AE6SW92G/Dwork et al. - 2012 - Fairness Through Awareness.pdf:application/pdf}
}

@misc{_google_2015,
	title = {Google {Photos} labeled black people 'gorillas'},
	url = {http://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/},
	abstract = {Google has apologized after its new Photos application identified black people as "gorillas."},
	urldate = {2016-11-11},
	journal = {USA TODAY},
	year = {2015},
	note = {bibtex: \_google\_2015},
	keywords = {bias example, read},
	file = {Snapshot:/home/hargup/zotero/storage/7EX4XPQW/29567465.html:text/html}
}

@article{valentino-devries_websites_2012,
	chapter = {Tech},
	title = {Websites {Vary} {Prices}, {Deals} {Based} on {Users}' {Information}},
	issn = {0099-9660},
	url = {http://www.wsj.com/articles/SB10001424127887323777204578189391813881534},
	abstract = {A Wall Street Journal investigation identified several companies, including Staples, Discover Financial Services, Rosetta Stone and Home Depot, that were consistently adjusting prices and displaying different product offers on their websites based on a range of characteristics that could be discovered about the Internet shopper, such as geographic location.},
	urldate = {2016-11-11},
	journal = {Wall Street Journal},
	author = {Valentino-DeVries, Jennifer and Singer-Vine, Jeremy and Soltani, Ashkan},
	month = dec,
	year = {2012},
	keywords = {bias example, book, gift stores, holiday 2012, holiday sales, office supplies, office supplies stores, read, retail, specialty stores, wholesale, wholesalers},
	file = {Snapshot:/home/hargup/zotero/storage/QQGDRGTS/SB10001424127887323777204578189391813881534.html:text/html}
}

@misc{hardt_how_2014,
	title = {How big data is unfair – {Moritz} {Hardt}},
	url = {https://medium.com/@mrtz/how-big-data-is-unfair-9aa544d739de#.thsimq5dg},
	abstract = {Understanding sources of unfairness in data driven decision making},
	urldate = {2016-11-11},
	journal = {Medium},
	author = {Hardt, Moritz},
	month = sep,
	year = {2014},
	keywords = {bias example, read},
	file = {Snapshot:/home/hargup/zotero/storage/EF9J994S/how-big-data-is-unfair-9aa544d739de.html:text/html}
}

@inproceedings{pedreschi_integrating_2009,
	address = {New York, NY, USA},
	series = {{ICAIL} '09},
	title = {Integrating {Induction} and {Deduction} for {Finding} {Evidence} of {Discrimination}},
	isbn = {978-1-60558-597-0},
	url = {http://doi.acm.org/10.1145/1568234.1568252},
	doi = {10.1145/1568234.1568252},
	abstract = {Automatic Decision Support Systems (DSS) are widely adopted for screening purposes in socially sensitive tasks, including access to credit, mortgage, insurance, labor market and other benefits. While less arbitrary decisions can potentially be guaranteed, automatic DSS can still be discriminating in the socially negative sense of resulting in unfair or unequal treatment of people. We present a reference model for finding (prima facie) evidence of discrimination in automatic DSS which is driven by a few key legal concepts. First, frequent classification rules are extracted from the set of decisions taken by the DSS over an input pool dataset. Key legal concepts are then used to drive the analysis of the set of classification rules, with the aim of discovering patterns of discrimination. We present an implementation, called LP2DD, of the overall reference model integrating induction, through data mining classification rule extraction, and deduction, through a computational logic implementation of the analytical tools.},
	urldate = {2016-11-11},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {ACM},
	author = {Pedreschi, Dino and Ruggieri, Salvatore and Turini, Franco},
	year = {2009},
	keywords = {classification, Data mining, direct and systematic discrimination, logic programming, scoring systems, to read},
	pages = {157--166},
	file = {ACM Full Text PDF:/home/hargup/zotero/storage/PQHUBH33/Pedreschi et al. - 2009 - Integrating Induction and Deduction for Finding Ev.pdf:application/pdf}
}

@article{zemel_learning_2013,
	title = {Learning {Fair} {Representations}.},
	volume = {28},
	url = {http://www.jmlr.org/proceedings/papers/v28/zemel13.pdf},
	urldate = {2016-11-11},
	journal = {ICML (3)},
	author = {Zemel, Richard S. and Wu, Yu and Swersky, Kevin and Pitassi, Toniann and Dwork, Cynthia},
	year = {2013},
	pages = {325--333},
	file = {Learning Fair Representations - zemel13.pdf:/home/hargup/zotero/storage/5QG4T2WP/zemel13.pdf:application/pdf}
}

@inproceedings{hannak_measuring_2014,
	address = {New York, NY, USA},
	series = {{IMC} '14},
	title = {Measuring {Price} {Discrimination} and {Steering} on {E}-commerce {Web} {Sites}},
	isbn = {978-1-4503-3213-2},
	url = {http://doi.acm.org/10.1145/2663716.2663744},
	doi = {10.1145/2663716.2663744},
	abstract = {Today, many e-commerce websites personalize their content, including Netflix (movie recommendations), Amazon (product suggestions), and Yelp (business reviews). In many cases, personalization provides advantages for users: for example, when a user searches for an ambiguous query such as ``router,'' Amazon may be able to suggest the woodworking tool instead of the networking device. However, personalization on e-commerce sites may also be used to the user's disadvantage by manipulating the products shown (price steering) or by customizing the prices of products (price discrimination). Unfortunately, today, we lack the tools and techniques necessary to be able to detect such behavior. In this paper, we make three contributions towards addressing this problem. First, we develop a methodology for accurately measuring when price steering and discrimination occur and implement it for a variety of e-commerce web sites. While it may seem conceptually simple to detect differences between users' results, accurately attributing these differences to price discrimination and steering requires correctly addressing a number of sources of noise. Second, we use the accounts and cookies of over 300 real-world users to detect price steering and discrimination on 16 popular e-commerce sites. We find evidence for some form of personalization on nine of these e-commerce sites. Third, we investigate the effect of user behaviors on personalization. We create fake accounts to simulate different user features including web browser/OS choice, owning an account, and history of purchased or viewed products. Overall, we find numerous instances of price steering and discrimination on a variety of top e-commerce sites.},
	urldate = {2016-11-11},
	booktitle = {Proceedings of the 2014 {Conference} on {Internet} {Measurement} {Conference}},
	publisher = {ACM},
	author = {Hannak, Aniko and Soeller, Gary and Lazer, David and Mislove, Alan and Wilson, Christo},
	year = {2014},
	keywords = {bias example, e-commerce, personalization, price discrimination, search},
	pages = {305--318},
	file = {ACM Full Text PDF:/home/hargup/zotero/storage/G5Z9REET/Hannak et al. - 2014 - Measuring Price Discrimination and Steering on E-c.pdf:application/pdf}
}

@inproceedings{barford_adscape:_2014,
	address = {New York, NY, USA},
	series = {{WWW} '14},
	title = {Adscape: {Harvesting} and {Analyzing} {Online} {Display} {Ads}},
	isbn = {978-1-4503-2744-2},
	shorttitle = {Adscape},
	url = {http://doi.acm.org/10.1145/2566486.2567992},
	doi = {10.1145/2566486.2567992},
	urldate = {2016-11-11},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {World} {Wide} {Web}},
	publisher = {ACM},
	author = {Barford, Paul and Canadi, Igor and Krushevskaja, Darja and Ma, Qiang and Muthukrishnan, S.},
	year = {2014},
	keywords = {bias example, measurement, targeted advertising, user profiles},
	pages = {597--608},
	file = {ACM Full Text PDF:/home/hargup/zotero/storage/U7AI9KW4/Barford et al. - 2014 - Adscape Harvesting and Analyzing Online Display A.pdf:application/pdf}
}

@article{lee_rodgers_thirteen_1988,
	title = {Thirteen {Ways} to {Look} at the {Correlation} {Coefficient}},
	volume = {42},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1988.10475524},
	doi = {10.1080/00031305.1988.10475524},
	language = {en},
	number = {1},
	urldate = {2016-11-11},
	journal = {The American Statistician},
	author = {Lee Rodgers, Joseph and Nicewander, W. Alan},
	month = feb,
	year = {1988},
	keywords = {to read},
	pages = {59--66},
	file = {10.1080@00031305.1988.10475524.pdf:/home/hargup/Downloads/10.1080@00031305.1988.10475524.pdf:application/pdf}
}

@inproceedings{zliobaite_handling_2011,
	title = {Handling {Conditional} {Discrimination}},
	doi = {10.1109/ICDM.2011.72},
	abstract = {Historical data used for supervised learning may contain discrimination. We study how to train classifiers on such data, so that they are discrimination free with respect to a given sensitive attribute, e.g., gender. Existing techniques that deal with this problem aim at removing all discrimination and do not take into account that part of the discrimination may be explainable by other attributes, such as, e.g., education level. In this context, we introduce and analyze the issue of conditional non-discrimination in classifier design. We show that some of the differences in decisions across the sensitive groups can be explainable and hence tolerable. We observe that in such cases, the existing discrimination aware techniques will introduce a reverse discrimination, which is undesirable as well. Therefore, we develop local techniques for handling conditional discrimination when one of the attributes is considered to be explanatory. Experimental evaluation demonstrates that the new local techniques remove exactly the bad discrimination, allowing differences in decisions as long as they are explainable.},
	booktitle = {2011 {IEEE} 11th {International} {Conference} on {Data} {Mining}},
	author = {Žliobaite, I. and Kamiran, F. and Calders, T.},
	month = dec,
	year = {2011},
	keywords = {classification, Computer science, Correlation, data handling, Data mining, Data models, Decision making, discrimination, Educational institutions, education level, handling conditional discrimination, historical data, independence, learning (artificial intelligence), Remuneration, supervised learning},
	pages = {992--1001},
	file = {IEEE Xplore Abstract Record:/home/hargup/zotero/storage/XGSJ8CQG/6137304.html:text/html;IEEE Xplore Full Text PDF:/home/hargup/zotero/storage/BZ8RZ4JX/Žliobaite et al. - 2011 - Handling Conditional Discrimination.pdf:application/pdf}
}

@article{kamiran_data_2012,
	title = {Data preprocessing techniques for classification without discrimination},
	volume = {33},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-011-0463-8},
	doi = {10.1007/s10115-011-0463-8},
	language = {en},
	number = {1},
	urldate = {2016-11-11},
	journal = {Knowledge and Information Systems},
	author = {Kamiran, Faisal and Calders, Toon},
	month = oct,
	year = {2012},
	pages = {1--33},
	file = {art%3A10.1007%2Fs10115-011-0463-8.pdf:/home/hargup/zotero/storage/IRAKPU4W/art%3A10.1007%2Fs10115-011-0463-8.pdf:application/pdf}
}

@article{qureshi_causal_2016,
	title = {Causal {Discrimination} {Discovery} {Through} {Propensity} {Score} {Analysis}},
	url = {https://arxiv.org/abs/1608.03735},
	urldate = {2016-11-11},
	journal = {arXiv preprint arXiv:1608.03735},
	author = {Qureshi, Bilal and Kamiran, Faisal and Karim, Asim and Ruggieri, Salvatore},
	year = {2016},
	file = {1608.03735.pdf:/home/hargup/zotero/storage/AJXDKI2Z/1608.03735.pdf:application/pdf}
}

@inproceedings{kamiran_classifying_2009,
	title = {Classifying without discriminating},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4909197},
	urldate = {2016-11-11},
	booktitle = {Computer, {Control} and {Communication}, 2009. {IC}4 2009. 2nd {International} {Conference} on},
	publisher = {IEEE},
	author = {Kamiran, Faisal and Calders, Toon},
	year = {2009},
	pages = {1--6},
	file = {untitled - download:/home/hargup/zotero/storage/2V3V88QG/download.pdf:application/pdf}
}

@inproceedings{kamiran_discrimination_2010,
	title = {Discrimination aware decision tree learning},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5694053},
	urldate = {2016-11-11},
	booktitle = {2010 {IEEE} {International} {Conference} on {Data} {Mining}},
	publisher = {IEEE},
	author = {Kamiran, Faisal and Calders, Toon and Pechenizkiy, Mykola},
	year = {2010},
	pages = {869--874},
	file = {6f3e8216cd7fc81de398a728ec421c684a69.pdf:/home/hargup/zotero/storage/Q8UEMPJE/6f3e8216cd7fc81de398a728ec421c684a69.pdf:application/pdf}
}

@book{custers_discrimination_2013,
	address = {Berlin, Heidelberg},
	series = {Studies in {Applied} {Philosophy}, {Epistemology} and {Rational} {Ethics}},
	title = {Discrimination and {Privacy} in the {Information} {Society}},
	volume = {3},
	isbn = {978-3-642-30486-6 978-3-642-30487-3},
	url = {http://link.springer.com/10.1007/978-3-642-30487-3},
	urldate = {2016-11-11},
	publisher = {Springer Berlin Heidelberg},
	editor = {Custers, Bart and Calders, Toon and Schermer, Bart and Zarsky, Tal},
	year = {2013},
	file = {SAPERE 3 - Discrimination and Privacy in the Information Society - Bart Custers, Toon Calders, Bart Schermer, Tal Zarsky (eds.) Discrimination and Privacy in the Information Society_ Data Mining and Profiling in Large Databases 2013.pdf:/home/hargup/zotero/storage/D8ZET7JM/Bart Custers,  Toon Calders,  Bart Schermer,  Tal Zarsky  (eds.) Discrimination and Privacy in t.pdf:application/pdf}
}

@techreport{bird_exploring_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Exploring or {Exploiting}? {Social} and {Ethical} {Implications} of {Autonomous} {Experimentation} in {AI}},
	shorttitle = {Exploring or {Exploiting}?},
	url = {https://papers.ssrn.com/abstract=2846909},
	abstract = {In the field of computer science, large-scale experimentation on users is not new. However, driven by advances in artificial intelligence, novel autonomous systems for experimentation are emerging that raise complex, unanswered questions for the field. Some of these questions are computational, while others relate to the social and ethical implications of these systems. We see these normative questions as urgent because they pertain to critical infrastructure upon which large populations depend, such as transportation and healthcare. Although experimentation on widely used online platforms like Facebook has stoked controversy in recent years, the unique risks posed by autonomous experimentation have not received sufficient attention, even though such techniques are being trialled on a massive scale. In this paper, we identify several questions about the social and ethical implications of autonomous experimentation systems. These questions concern the design of such systems, their effects on users, and their resistance to some common mitigations.},
	number = {ID 2846909},
	urldate = {2016-11-11},
	institution = {Social Science Research Network},
	author = {Bird, Sarah and Barocas, Solon and Crawford, Kate and Diaz, Fernando and Wallach, Hanna},
	month = oct,
	year = {2016},
	keywords = {Artificial Intelligence, Autonomous Experimentation, Ethics, Privacy},
	file = {Snapshot:/home/hargup/zotero/storage/E42PRJRP/papers.html:text/html;SSRN-id2846909.pdf:/home/hargup/Downloads/SSRN-id2846909.pdf:application/pdf}
}

@article{ben-aaron_transparency_2016,
	title = {Transparency by {Conformity}: {A} {Field} {Experiment} {Evaluating} {Openness} in {Local} {Governments}},
	issn = {1540-6210},
	shorttitle = {Transparency by {Conformity}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/puar.12596/abstract},
	doi = {10.1111/puar.12596},
	abstract = {Sunshine laws establishing government transparency are ubiquitous in the United States; however, the intended degree of openness is often unclear or unrealized. Although researchers have identified characteristics of government organizations or officials that affect the fulfillment of public records requests, they have not considered the influence that government organizations have on one another. This picture of independently acting organizations does not accord with the literature on diffusion in public policy and administration. This article presents a field experiment testing whether a county government's fulfillment of a public records request is influenced by the knowledge that its peers have already complied. The authors propose that knowledge of peer compliance should induce competitive pressures to comply and resolve legal ambiguity in favor of compliance. Findings indicate peer conformity affects both in the time to initial response and in the rate of complete request fulfillment.},
	language = {en},
	urldate = {2016-11-11},
	journal = {Public Administration Review},
	author = {ben-Aaron, James and Denny, Matthew and Desmarais, Bruce and Wallach, Hanna},
	month = jul,
	year = {2016},
	pages = {n/a--n/a},
	file = {Full Text PDF:/home/hargup/zotero/storage/E3E9BF7C/ben-Aaron et al. - 2016 - Transparency by Conformity A Field Experiment Eva.pdf:application/pdf;Snapshot:/home/hargup/zotero/storage/UTPU94NR/abstract.html:text/html}
}

@misc{vaughan_inescapability_2016,
	title = {The {Inescapability} of {Uncertainty} – {Jennifer} {Wortman} {Vaughan}},
	url = {https://medium.com/@jennwv/uncertainty-edd5caf8981b},
	abstract = {AI, Uncertainty, and Why You Should Vote No Matter What Predictions Say},
	urldate = {2016-11-11},
	journal = {Medium},
	author = {Vaughan, Jennifer Wortman},
	month = oct,
	year = {2016},
	keywords = {read},
	file = {Snapshot:/home/hargup/zotero/storage/DRATVTS4/uncertainty-edd5caf8981b.html:text/html}
}

@misc{_computer_2016,
	title = {A computer program used for bail and sentencing decisions was labeled biased against blacks. {It}’s actually not that clear.},
	url = {https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/},
	abstract = {The tool called COMPAS may be biased. But it's hard to tell.},
	urldate = {2016-11-12},
	journal = {Washington Post},
	month = oct,
	year = {2016},
	note = {bibtex: \_computer\_2016},
	keywords = {read, rebuttal},
	file = {Snapshot:/home/hargup/zotero/storage/RR5N2H4Q/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas.html:text/html}
}

@article{chaney_detecting_????,
	title = {Detecting and {Characterizing} {Events}},
	url = {http://ajbc.io/projects/papers/ChaneyWallachConnellyBlei2016.pdf},
	urldate = {2016-11-12},
	author = {Chaney, Allison JB and Wallach, Hanna and Connelly, Matthew and Blei, David M.},
	file = {chaney16detecting.pdf:/home/hargup/zotero/storage/BA82DE72/chaney16detecting.pdf:application/pdf}
}

@misc{_only_????,
	title = {Only {Facebook} knows how it spreads fake election news},
	url = {http://www.cbc.ca/news/technology/facebook-fake-news-us-election-algorithm-transparency-1.3846073},
	abstract = {Whether fake news stories shared to Facebook affected voter opinion is impossible to say because of how little insight we have into how Facebook’s myriad algorithms work.},
	urldate = {2016-11-12},
	journal = {CBC News},
	file = {Snapshot:/home/hargup/zotero/storage/QKCBV6ST/facebook-fake-news-us-election-algorithm-transparency-1.html:text/html}
}

@inproceedings{mikians_detecting_2012,
	title = {Detecting price and search discrimination on the internet},
	url = {http://dl.acm.org/citation.cfm?id=2390245},
	urldate = {2016-11-12},
	booktitle = {Proceedings of the 11th {ACM} {Workshop} on {Hot} {Topics} in {Networks}},
	publisher = {ACM},
	author = {Mikians, Jakub and Gyarmati, László and Erramilli, Vijay and Laoutaris, Nikolaos},
	year = {2012},
	pages = {79--84},
	file = {hotnets12-final94.pdf:/home/hargup/zotero/storage/I2BS9HAC/hotnets12-final94.pdf:application/pdf}
}

@techreport{abrams_judges_2013,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Do {Judges} {Vary} in {Their} {Treatment} of {Race}?},
	url = {https://papers.ssrn.com/abstract=1800840},
	abstract = {Are minorities treated differently by the legal system?  Systematic racial differences in case characteristics, many unobservable, make this a difficult question to answer directly.  In this paper, we estimate whether judges differ from each other in how they sentence minorities, avoiding potential bias from unobservable case characteristics by exploiting the random assignment of cases to judges.  We measure the between-judge variation in the difference in incarceration rates and sentence lengths between African-American and White defendants.  We perform a Monte Carlo simulation in order to explicitly construct the appropriate counterfactual, where race does not influence judicial sentencing. In our data set, which includes felony cases from Cook County, Illinois, we find statistically significant between-judge variation in incarceration rates, although not in sentence lengths.},
	number = {ID 1800840},
	urldate = {2016-11-12},
	institution = {Social Science Research Network},
	author = {Abrams, David and Bertrand, Marianne and Mullainathan, Sendhil},
	month = may,
	year = {2013},
	keywords = {accounting for disparities in judicial behavior, bias, Criminal law, empirical research, forecasting and simulation, incarceration rate, law and economics of crime, Monte Carlo simulation, punishment, race and justice, racial discrimination, sentence length, sentencing},
	file = {Snapshot:/home/hargup/zotero/storage/B3MMKH9C/papers.html:text/html}
}

@techreport{harcourt_risk_2010,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Risk as a {Proxy} for {Race}},
	url = {https://papers.ssrn.com/abstract=1677654},
	abstract = {Today, an increasing chorus argues that risk-assessment instruments are a politically feasible way to resolve our problem of mass incarceration and reduce prison populations. In this essay, I argue against this progressive argument for prediction:  using risk-assessment tools to decrease prison populations would unquestionably aggravate the already intolerable racial imbalance in our prison populations and will not address the real source of mass incarceration, namely the admissions process. Risk has collapsed into prior criminal history, and prior criminal history has become a proxy for race. This means that using risk-assessment tools, even for progressive ends, is going to significantly aggravate the already unacceptable racial disparities in our criminal justice system. Instead of turning to prediction, we need to address prison admissions. Recent evidence suggests that our carceral excess was not so much fueled by the length of sentences, as it was by the front end: new admissions. The real solution to mass incarceration, then, is not to cut short prison terms though prediction, but to reduce admissions to prison.},
	number = {ID 1677654},
	urldate = {2016-11-12},
	institution = {Social Science Research Network},
	author = {Harcourt, Bernard E.},
	month = sep,
	year = {2010},
	keywords = {Actuarial Methods, Criminal Justice, Deinstitutionalization, Imprisonment, Mass Incarceration, Prediction, Prediction Instruments, Prior Criminal History, Race, racial discrimination, Risk, Risk-Assessment, Selective Incapacitation},
	file = {Snapshot:/home/hargup/zotero/storage/SIFPFWDW/papers.html:text/html}
}

@techreport{skeem_risk_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Risk, {Race}, \& {Recidivism}: {Predictive} {Bias} and {Disparate} {Impact}},
	shorttitle = {Risk, {Race}, \& {Recidivism}},
	url = {https://papers.ssrn.com/abstract=2687339},
	abstract = {One way to unwind mass incarceration without compromising public safety is to use risk assessment instruments in sentencing and corrections. Although these instruments figure prominently in current reforms, critics argue that benefits in crime control will be offset by an adverse effect on racial minorities.  Based on a sample of 34,794 federal offenders, we examine the relationships among race, risk assessment (the Post Conviction Risk Assessment [PCRA]), and future arrest.  First, application of well-established principles of psychological science revealed little evidence of test bias for the PCRA — the instrument strongly predicts arrest for both Black and White offenders and a given score has essentially the same meaning — i.e., same probability of recidivism — across groups. Second, Black offenders obtain higher average PCRA scores than White offenders (d= 0.34; 13.5\% non-overlap in groups’ scores), so some applications could create disparate impact.  Third, most (66\%) of the racial difference in PCRA scores is attributable to criminal history — which is already embedded in sentencing guidelines.  Finally, criminal history is not a proxy for race, but instead mediates the relationship between race and future arrest .  Data are more helpful than rhetoric, if the goal is to improve practice at this opportune moment in history.},
	number = {ID 2687339},
	urldate = {2016-11-12},
	institution = {Social Science Research Network},
	author = {Skeem, Jennifer L. and Lowenkamp, Christopher T.},
	month = jun,
	year = {2016},
	keywords = {disparities, Race, risk assessment, sentencing, test bias},
	file = {Snapshot:/home/hargup/zotero/storage/G35HRPQM/papers.html:text/html}
}

@misc{_flickr_2015,
	title = {Flickr {Fixing} '{Racist}' {Auto}-{Tagging} {Feature} {After} {Black} {Man} {Mislabeled} '{Ape}'},
	url = {http://petapixel.com/2015/05/20/flickr-fixing-racist-auto-tagging-feature-after-black-man-mislabeled-ape/},
	abstract = {Flickr has caused some unexpected controversy with the auto-tagging feature it launched earlier this month. The "advanced image recognition" system has bee},
	urldate = {2016-11-12},
	journal = {PetaPixel},
	month = may,
	year = {2015},
	keywords = {bias example, read},
	file = {Snapshot:/home/hargup/zotero/storage/8ZMF235U/flickr-fixing-racist-auto-tagging-feature-after-black-man-mislabeled-ape.html:text/html}
}

@misc{frucci_hp_2009,
	title = {{HP} {Face}-{Tracking} {Webcams} {Don}'t {Recognize} {Black} {People}},
	url = {http://gizmodo.com/5431190/hp-face-tracking-webcams-dont-recognize-black-people},
	abstract = {This is awkward. It appears that HP\&\#39;s new webcams, which have facial-tracking software, can\&\#39;t recognize black faces, as evidenced in the above video. HP has responded:},
	urldate = {2016-11-12},
	journal = {Gizmodo},
	author = {Frucci, Adam},
	year = {2009},
	note = {bibtex: frucci\_hp\_2009},
	keywords = {bias example, read},
	file = {Snapshot:/home/hargup/zotero/storage/74HCTCAF/hp-face-tracking-webcams-dont-recognize-black-people.html:text/html}
}

@misc{mattu_how_2016,
	title = {How {We} {Analyzed} the {COMPAS} {Recidivism} {Algorithm}},
	url = {https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm},
	abstract = {ProPublica is an independent, non-profit newsroom that produces investigative journalism in the public interest.},
	urldate = {2016-11-12},
	journal = {ProPublica},
	author = {Mattu, Julia Angwin, Lauren Kirchner, Surya, Jeff Larson},
	month = may,
	year = {2016},
	file = {Snapshot:/home/hargup/zotero/storage/MDHH66RE/how-we-analyzed-the-compas-recidivism-algorithm.html:text/html}
}

@misc{eveleth_inherent_2016,
	title = {The {Inherent} {Bias} of {Facial} {Recognition}},
	url = {http://motherboard.vice.com/read/the-inherent-bias-of-facial-recognition},
	abstract = {The fact that algorithms can contain latent biases is becoming clearer and clearer. And some people saw this coming.},
	urldate = {2016-11-12},
	journal = {Motherboard},
	author = {Eveleth, Rose},
	month = mar,
	year = {2016},
	note = {bibtex: eveleth\_inherent\_????},
	keywords = {read},
	file = {Snapshot:/home/hargup/zotero/storage/Q3S9FUDT/the-inherent-bias-of-facial-recognition.html:text/html}
}

@article{dedeo_wrong_2014,
	title = {Wrong side of the tracks: {Big} {Data} and {Protected} {Categories}},
	shorttitle = {Wrong side of the tracks},
	url = {http://arxiv.org/abs/1412.4643},
	abstract = {When we use machine learning for public policy, we find that many useful variables are associated with others on which it would be ethically problematic to base decisions. This problem becomes particularly acute in the Big Data era, when predictions are often made in the absence of strong theories for underlying causal mechanisms. We describe the dangers to democratic decision-making when high-performance algorithms fail to provide an explicit account of causation. We then demonstrate how information theory allows us to degrade predictions so that they decorrelate from protected variables with minimal loss of accuracy. Enforcing total decorrelation is at best a near-term solution, however. The role of causal argument in ethical debate urges the development of new, interpretable machine-learning algorithms that reference causal mechanisms.},
	urldate = {2016-11-12},
	journal = {arXiv:1412.4643 [physics, stat]},
	author = {DeDeo, Simon},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.4643},
	keywords = {Computer Science - Computers and Society, Computer Science - Information Theory, Physics - Physics and Society, Statistics - Methodology},
	file = {arXiv\:1412.4643 PDF:/home/hargup/zotero/storage/6DWNPE4U/DeDeo - 2014 - Wrong side of the tracks Big Data and Protected C.pdf:application/pdf;arXiv.org Snapshot:/home/hargup/zotero/storage/QAUGEV3I/1412.html:text/html}
}

@article{albarghouthi_fairness_2016,
	title = {Fairness as a {Program} {Property}},
	url = {http://arxiv.org/abs/1610.06067},
	abstract = {We explore the following question: Is a decision-making program fair, for some useful definition of fairness? First, we describe how several algorithmic fairness questions can be phrased as program verification problems. Second, we discuss an automated verification technique for proving or disproving fairness of decision-making programs with respect to a probabilistic model of the population.},
	urldate = {2016-11-12},
	journal = {arXiv:1610.06067 [cs]},
	author = {Albarghouthi, Aws and D'Antoni, Loris and Drews, Samuel and Nori, Aditya},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.06067},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
	file = {arXiv\:1610.06067 PDF:/home/hargup/zotero/storage/FIPJ3D9U/Albarghouthi et al. - 2016 - Fairness as a Program Property.pdf:application/pdf;arXiv.org Snapshot:/home/hargup/zotero/storage/ZD6UHBFA/1610.html:text/html}
}

@misc{_fda_????,
	title = {An {FDA} for {Algorithms} by {Andrew} {Tutt} :: {SSRN}},
	url = {https://papers.ssrn.com/sol3/Papers.cfm?abstract_id=2747994},
	urldate = {2016-11-12},
	file = {An FDA for Algorithms by Andrew Tutt \:\: SSRN:/home/hargup/zotero/storage/7PQVIS84/Papers.html:text/html}
}

@misc{tynan_algorithms_????,
	title = {Algorithms, {Intelligence}, and {Learning} {Oh} {My} {\textbar} {Privacy} {International}},
	url = {https://privacyinternational.org/node/990},
	urldate = {2016-11-13},
	author = {Tynan, Richard},
	note = {bibtex: ty\_algorithms\_????},
	file = {Algorithms, Intelligence, and Learning Oh My | Privacy International:/home/hargup/zotero/storage/QNUC27NF/990.html:text/html}
}

@misc{_algorithms_????,
	title = {Algorithms, {Intelligence}, and {Learning} {Oh} {My} {\textbar} {Privacy} {International}},
	url = {https://privacyinternational.org/node/990},
	urldate = {2016-11-13},
	file = {Algorithms, Intelligence, and Learning Oh My | Privacy International:/home/hargup/zotero/storage/IHCAIMQA/990.html:text/html}
}

@misc{oreilly_media_2016,
	title = {Media in the {Age} of {Algorithms}},
	url = {https://medium.com/@timoreilly/media-in-the-age-of-algorithms-63e80b9b0a73#.kiohn9rbs},
	abstract = {Since Tuesday’s election, there’s been a lot of finger pointing, and many of those fingers are pointing at Facebook, arguing that their…},
	urldate = {2016-11-13},
	journal = {Medium},
	author = {O'Reilly, Tim},
	month = nov,
	year = {2016},
	file = {Snapshot:/home/hargup/zotero/storage/XMHTPZPX/media-in-the-age-of-algorithms-63e80b9b0a73.html:text/html}
}

@misc{_algorithms_????-1,
	title = {Algorithms, {Intelligence}, and {Learning} {Oh} {My} {\textbar} {Privacy} {International}},
	url = {https://privacyinternational.org/node/990},
	urldate = {2016-11-13},
	file = {Algorithms, Intelligence, and Learning Oh My | Privacy International:/home/hargup/zotero/storage/CHBKAMAC/990.html:text/html}
}

@misc{dubey_jumla_2016,
	title = {Jumla is the new black},
	url = {https://chunauti.org/2016/11/13/jumla-is-the-new-black/},
	abstract = {It’s hard to tell whether demonetisation will turn out to be a bold hit or a costly miss, but it appears to many that the government has taken the black money bull by the horns. Announcing th…},
	urldate = {2016-11-13},
	journal = {chunauti},
	author = {Dubey, Amitabh},
	month = nov,
	year = {2016},
	file = {Snapshot:/home/hargup/zotero/storage/64QNKTCX/jumla-is-the-new-black.html:text/html}
}

@misc{_tipu_2016,
	title = {Tipu {Sultan} {Jayanthi} – {A} {Celebration} of {Bigotry} and {Barbarity}},
	url = {http://indiafacts.org/tipu-sultan-jayanthi-celebration-bigotry-barbarity/},
	abstract = {Each action of Tipu Sultan against the infidels that we list has been usually confirmed by multiple sources, both Indian and European.},
	urldate = {2016-11-13},
	journal = {IndiaFacts},
	month = nov,
	year = {2016},
	file = {Snapshot:/home/hargup/zotero/storage/99MITDW7/tipu-sultan-jayanthi-celebration-bigotry-barbarity.html:text/html}
}

@misc{_academic_2016,
	title = {Academic {Hinduphobia} {Going} {Mainstream}},
	url = {http://indiafacts.org/academic-hinduphobia-going-mainstream/},
	abstract = {We would urge all pseudo academicians who profess to know India better than Indians themselves, to let us be.},
	urldate = {2016-11-13},
	journal = {IndiaFacts},
	month = oct,
	year = {2016},
	file = {Snapshot:/home/hargup/zotero/storage/WHXZMN82/academic-hinduphobia-going-mainstream.html:text/html}
}

@misc{_sarvadharma_2016,
	title = {Sarvadharma {Samabhava} of the {Abrahamics}},
	url = {http://indiafacts.org/sarvadharma-samabhava-of-the-abrahamics/},
	abstract = {As a Hindu, ask yourself again this simple question.  If Christianity and Islam respect Hinduism as “equal”, why do they need to convert or kill us?},
	urldate = {2016-11-13},
	journal = {IndiaFacts},
	month = jan,
	year = {2016},
	file = {Snapshot:/home/hargup/zotero/storage/NI939JU7/sarvadharma-samabhava-of-the-abrahamics.html:text/html}
}

@misc{_open_2013,
	title = {Open {Letter} to {Pope} {Francis}},
	url = {http://indiafacts.org/open-letter-to-pope-francis/},
	abstract = {Respected Holy Father, Great hope for a positive change in the Catholic Church is pinned...},
	urldate = {2016-11-13},
	journal = {IndiaFacts},
	month = dec,
	year = {2013},
	file = {Snapshot:/home/hargup/zotero/storage/EW5N4PCR/open-letter-to-pope-francis.html:text/html}
}

@book{jain_indian_2015,
	edition = {7th Edition},
	title = {Indian {Constitutional} {Law}},
	isbn = {978-93-5143-064-3},
	publisher = {Lexis Nexis},
	author = {Jain, M P},
	year = {2015},
	note = {bibtex: jain\_indian\_????},
	keywords = {law}
}

@article{kleinberg_inherent_2016,
	title = {Inherent {Trade}-{Offs} in the {Fair} {Determination} of {Risk} {Scores}},
	url = {http://arxiv.org/abs/1609.05807},
	abstract = {Recent discussion in the public sphere about algorithmic classification has involved tension between competing notions of what it means for a probabilistic classification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them.},
	urldate = {2016-11-14},
	journal = {arXiv:1609.05807 [cs, stat]},
	author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.05807
bibtex: kleinberg\_inherent\_2016},
	keywords = {Computer Science - Computers and Society, Computer Science - Learning, Statistics - Machine Learning, theory, to read},
	file = {arXiv.org Snapshot:/home/hargup/zotero/storage/H4GGGFF3/1609.html:text/html;Kleinberg et al_2016_Inherent Trade-Offs in the Fair Determination of Risk Scores.pdf:/home/hargup/zotero/storage/3PBEC7B8/Kleinberg et al_2016_Inherent Trade-Offs in the Fair Determination of Risk Scores.pdf:application/pdf}
}

@misc{larson_propublica_2016,
	title = {{ProPublica} {Responds} to {Company}’s {Critique} of {Machine} {Bias} {Story}},
	url = {https://www.propublica.org/article/propublica-responds-to-companys-critique-of-machine-bias-story},
	abstract = {Northpointe asserts that a software program it sells that predicts the likelihood a person will commit future crimes is equally fair to black and white defendants. We re-examined the data, considered the company’s criticisms, and stand by our conclusions.},
	urldate = {2016-11-14},
	journal = {ProPublica},
	author = {Larson, Jeff, Julia Angwin},
	month = jul,
	year = {2016},
	note = {bibtex: larson\_propublica\_2016},
	file = {Snapshot:/home/hargup/zotero/storage/5FFI64V8/propublica-responds-to-companys-critique-of-machine-bias-story.html:text/html}
}

@misc{larson_propublica_2016-1,
	title = {{ProPublica} {Responds} to {Company}’s {Critique} of {Machine} {Bias} {Story}},
	url = {https://www.propublica.org/article/propublica-responds-to-companys-critique-of-machine-bias-story},
	abstract = {Northpointe asserts that a software program it sells that predicts the likelihood a person will commit future crimes is equally fair to black and white defendants. We re-examined the data, considered the company’s criticisms, and stand by our conclusions.},
	urldate = {2016-11-14},
	journal = {ProPublica},
	author = {Larson, Jeff, Julia Angwin},
	month = jul,
	year = {2016},
	note = {bibtex: larson\_propublica\_2016-1},
	file = {Snapshot:/home/hargup/zotero/storage/M3K9CXNU/propublica-responds-to-companys-critique-of-machine-bias-story.html:text/html}
}

@misc{angwin_technical_2016,
	title = {Technical {Response} to {Northpointe}},
	url = {https://www.propublica.org/article/technical-response-to-northpointe},
	abstract = {Northpointe asserts that a software program it sells that predicts the likelihood a person will commit future crimes is equally fair to black and white defendants. We re-examined the data, considered the company’s criticisms, and stand by our conclusions.},
	urldate = {2016-11-14},
	journal = {ProPublica},
	author = {Angwin, Julia, Jeff Larson},
	month = jul,
	year = {2016},
	note = {bibtex: angwin\_technical\_2016},
	file = {Snapshot:/home/hargup/zotero/storage/A4JFRFUV/technical-response-to-northpointe.html:text/html}
}

@article{hardt_equality_2016,
	title = {Equality of {Opportunity} in {Supervised} {Learning}},
	url = {http://arxiv.org/abs/1610.02413},
	abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. We illustrate our notion using a case study of FICO credit scores.},
	urldate = {2016-11-14},
	journal = {arXiv:1610.02413 [cs]},
	author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.02413
bibtex: hardt\_equality\_2016},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/home/hargup/zotero/storage/DZ4HT8G5/1610.html:text/html;Hardt et al_2016_Equality of Opportunity in Supervised Learning.pdf:/home/hargup/zotero/storage/NWRMAWGH/Hardt et al_2016_Equality of Opportunity in Supervised Learning.pdf:application/pdf}
}

@misc{hrala_excel_2016,
	title = {Excel {Is} to {Blame} for {Major} {Typos} in 20\% of {Scientific} {Papers} on {Genes}},
	url = {http://www.sciencealert.com/excel-is-responsible-for-20-percent-of-errors-in-genetic-scientific-papers},
	abstract = {Excel’s default format settings are partially responsible for errors in around 20 percent of scientific papers discussing genes, a new study has found.},
	urldate = {2016-11-14},
	journal = {ScienceAlert},
	author = {Hrala, Josh},
	month = aug,
	year = {2016},
	note = {bibtex: hrala\_excel\_????},
	file = {Snapshot:/home/hargup/zotero/storage/UA2FPSHC/excel-is-responsible-for-20-percent-of-errors-in-genetic-scientific-papers.html:text/html}
}

@misc{downey_recidivism_????,
	title = {Recidivism and single-case probabilities},
	url = {http://allendowney.blogspot.com/2015/11/recidivism-and-single-case-probabilities.html},
	abstract = {I am collaborating with a criminologist who studies recidivism.  In the context of crime statistics, a recidivist is a convicted criminal wh...},
	urldate = {2016-11-14},
	author = {Downey, Allen},
	note = {bibtex: downey\_recidivism\_????},
	file = {Snapshot:/home/hargup/zotero/storage/BUE8ZE5F/recidivism-and-single-case-probabilities.html:text/html}
}

@misc{_wikipedia_2010,
	title = {Wikipedia {Survey} – {Overview} of {Results}},
	url = {https://web.archive.org/web/20110728182835/http://www.wikipediastudy.org/docs/Wikipedia_Overview_15March2010-FINAL.pdf},
	urldate = {2016-11-14},
	month = mar,
	year = {2010},
	note = {bibtex: \_wikipedia\_2010},
	file = {2011_Wikipedia_Overview_15March2010-FINAL.pdf:/home/hargup/zotero/storage/8KBSD9UM/2011_Wikipedia_Overview_15March2010-FINAL.pdf:application/pdf}
}

@inproceedings{lam_wp:_2011,
	title = {{WP}: clubhouse?: an exploration of {Wikipedia}'s gender imbalance},
	shorttitle = {{WP}},
	url = {http://dl.acm.org/citation.cfm?id=2038560},
	urldate = {2016-11-14},
	booktitle = {Proceedings of the 7th international symposium on {Wikis} and open collaboration},
	publisher = {ACM},
	author = {Lam, Shyong Tony K. and Uduwage, Anuradha and Dong, Zhenhua and Sen, Shilad and Musicant, David R. and Terveen, Loren and Riedl, John},
	year = {2011},
	note = {bibtex: lam\_wp:\_2011},
	pages = {1--10},
	file = {wp-gender-wikisym2011.pdf:/home/hargup/zotero/storage/6HUCTM7S/wp-gender-wikisym2011.pdf:application/pdf}
}

@article{kamiran_data_2012-1,
	title = {Data preprocessing techniques for classification without discrimination},
	volume = {33},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-011-0463-8},
	doi = {10.1007/s10115-011-0463-8},
	language = {en},
	number = {1},
	urldate = {2016-11-14},
	journal = {Knowledge and Information Systems},
	author = {Kamiran, Faisal and Calders, Toon},
	month = oct,
	year = {2012},
	note = {bibtex: kamiran\_data\_2012-1},
	pages = {1--33},
	file = {art%3A10.1007%2Fs10115-011-0463-8.pdf:/home/hargup/zotero/storage/EVG45I5R/art%3A10.1007%2Fs10115-011-0463-8.pdf:application/pdf}
}

@article{graham_digital_2015,
	title = {Digital {Divisions} of {Labor} and {Informational} {Magnetism}: {Mapping} {Participation} in {Wikipedia}},
	volume = {105},
	issn = {0004-5608},
	shorttitle = {Digital {Divisions} of {Labor} and {Informational} {Magnetism}},
	url = {http://dx.doi.org/10.1080/00045608.2015.1072791},
	doi = {10.1080/00045608.2015.1072791},
	abstract = {There are now more than 3 billion Internet users on our planet. The connections afforded to all of those people, in theory, allow for an unprecedented amount of communication and public participation. The goal of this article is to examine how those potentials match up to actual patterns of participation. By focusing on Wikipedia, the world's largest and most used repository of user-generated content, we are able to gain important insights into the geographies of voice and participation. This article shows that the relative democratization of the Internet has not brought about a concurrent democratization of voice and participation. Despite the fact that it is widely used around the world, Wikipedia is characterized by highly uneven geographies of participation. The goal of highlighting these inequalities is not to suggest that they are insurmountable. Our regression analysis shows that the availability of broadband is a clear factor in the propensity of people to participate on Wikipedia. The relationship is not a linear one, though. As a country approaches levels of connectivity above about 450,000 broadband Internet connections, the ability of broadband access to positively affect participation keeps increasing. Complicating this issue is the fact that participation from the world's economic peripheries tends to focus on editing about the world's cores rather than their own local regions. These results ultimately point to an informational magnetism that is cast by the world's economic cores, virtuous and vicious cycles that make it difficult to reconfigure networks and hierarchies of knowledge production.},
	number = {6},
	urldate = {2016-11-14},
	journal = {Annals of the Association of American Geographers},
	author = {Graham, Mark and Straumann, Ralph K. and Hogan, Bernie},
	month = nov,
	year = {2015},
	note = {bibtex: graham\_digital\_2015},
	keywords = {digital divide, digital labor, divisoria digital, geografía de la información, information geography, participación, participation, trabajo digital, Wikipedia, 数码落差，数码劳动，信息地理学，参与，维基百科。},
	pages = {1158--1178},
	file = {Snapshot:/home/hargup/zotero/storage/WCBRPPIR/00045608.2015.html:text/html}
}